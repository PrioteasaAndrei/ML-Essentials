{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name, matrikelnummer\n",
    "\n",
    "Prioteasa Cristi Andrei, 4740844 \\\n",
    "..., ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BASE_MODEL = False\n",
    "TRAIN_DROPOUT_MODEL = True\n",
    "TRAIN_PRELU_MODEL = False\n",
    "TRAIN_LENET = False\n",
    "TRAIN_LENET_SHIFT = False\n",
    "\n",
    "ALL = TRAIN_BASE_MODEL and TRAIN_DROPOUT_MODEL and TRAIN_PRELU_MODEL and TRAIN_LENET and TRAIN_LENET_SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "transform_extended = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "]) \n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape).to(device) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data.to(device)\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "def model(x, w_h, w_h2, w_o):\n",
    "    h = rectify(x @ w_h)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BASE_MODEL:\n",
    "\n",
    "    # initialize weights\n",
    "    # input shape is (B, 784)\n",
    "    w_h = init_weights((784, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_h2 = init_weights((625, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_o = init_weights((625, 10))\n",
    "    # output shape is (B, 10)\n",
    "\n",
    "\n",
    "    optimizer = RMSprop(params=[w_h, w_h2, w_o])\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BASE_MODEL:\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            # our model requires flattened input\n",
    "            x = x.reshape(batch_size, 784).to(device)\n",
    "            # feed input through model\n",
    "            noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "                    x = x.reshape(batch_size, 784).to(device)\n",
    "                    noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_BASE_MODEL:\n",
    "    plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 2), test_loss, label=\"Test\")\n",
    "    plt.title(\"Train and Test Loss over Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dropout(X,p_drop=0.5):\n",
    "\n",
    "#     if p_drop > 1:\n",
    "#         return X\n",
    "\n",
    "#     mask = torch.bernoulli(torch.ones_like(X) * (1 - p_drop))\n",
    "#     return mask * X / (1 - p_drop)\n",
    "\n",
    "# def dropout_layer(X, dropout):\n",
    "#     assert 0 <= dropout <= 1\n",
    "#     if dropout == 1: return torch.zeros_like(X)\n",
    "#     mask = (torch.rand(X.shape) > dropout).float()\n",
    "#     return mask * X / (1.0 - dropout)\n",
    "\n",
    "# def dropout(X, p_drop=0.5):\n",
    "#     if 0 < p_drop < 1:\n",
    "#         dropout_mask = torch.random.binomial(1, 1 - p_drop, size=X.shape)\n",
    "#         X_dropout = X * dropout_mask / (1 - p_drop)\n",
    "#         return X_dropout\n",
    "#     else:\n",
    "#         return X\n",
    "    \n",
    "def dropout(X, p_drop=0.5):\n",
    "    if 0 < p_drop < 1:\n",
    "        dropout_mask = torch.bernoulli(torch.full(X.shape, 1 - p_drop)).to(X.device)\n",
    "        X_dropout = X * dropout_mask / (1 - p_drop)\n",
    "        return X_dropout\n",
    "    else:\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_model(x, w_h, w_h2, w_o, p_drop_input=0,p_drop_hidden=0.2):\n",
    "    x = dropout(x, p_drop_input)\n",
    "    h = rectify(x @ w_h)\n",
    "    h = dropout(h, p_drop_hidden)\n",
    "    h2 = rectify(h @ w_h2)\n",
    "    h2 = dropout(h2, p_drop_hidden)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_DROPOUT_MODEL:\n",
    "    # initialize weights\n",
    "    # input shape is (B, 784)\n",
    "    w_h_drop = init_weights((784, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_h2_drop = init_weights((625, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_o_drop = init_weights((625, 10))\n",
    "    # output shape is (B, 10)\n",
    "\n",
    "    optimizer = RMSprop(params=[w_h_drop, w_h2_drop, w_o_drop])\n",
    "\n",
    "    train_loss_drop = []\n",
    "    test_loss_drop = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Mean Train Loss: 4.59e-01\n",
      "Mean Test Loss:  1.95e-01\n",
      "Epoch: 2\n",
      "Mean Train Loss: 2.59e-01\n",
      "Mean Test Loss:  1.60e-01\n",
      "Epoch: 4\n",
      "Mean Train Loss: 2.62e-01\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_DROPOUT_MODEL:\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            # our model requires flattened input\n",
    "            x = x.reshape(batch_size, 784).to(device)\n",
    "            # feed input through model\n",
    "            noise_py_x = dropout_model(x, w_h_drop, w_h2_drop, w_o_drop)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_drop.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss_drop[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "                    x = x.reshape(batch_size, 784).to(device)\n",
    "                    noise_py_x = dropout_model(x, w_h_drop, w_h2_drop, w_o_drop,0,0)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            test_loss_drop.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss_drop[-1]:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_DROPOUT_MODEL:\n",
    "    plt.plot(np.arange(n_epochs + 1), train_loss_drop, label=\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 2), test_loss_drop, label=\"Test\")\n",
    "    plt.title(\"Train and Test Loss over Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout layer works by randomly deactivating certain neurons in the linear layers (or if applied to the input, deactivating certain features of the input vector). In this way the training load is more evenly distributed among the network, encourages diversity in the learned parameters and helps reduce redundancy i.e. neurons that do not significantly contribute to the performance of the network (cause of their existance can be attributed to overparametrization; see https://arxiv.org/abs/1503.02531, https://arxiv.org/abs/1803.03635).\n",
    "\n",
    "In our implementation we use all the neurons for the testing to get the best accuracy, so in this sense this is Inverted Dropout not dropout. See: https://stackoverflow.com/questions/54109617/implementing-dropout-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_DROPOUT_MODEL and TRAIN_BASE_MODEL:\n",
    "    ## test error comparation\n",
    "    print(\"With dropout: {}\".format(test_loss_drop[-1]))\n",
    "    print(\"Without dropout: {}\".format(test_loss[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parametric Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PRelu(X,a):\n",
    "    assert X.shape == a.shape\n",
    "\n",
    "    return torch.where(X > 0, X, a*X)\n",
    "\n",
    "# define the neural network\n",
    "def prelu_model(x, w_h, w_h2, w_o, a1, a2):\n",
    "    h = PRelu(x @ w_h,a1)\n",
    "    h2 = PRelu(h @ w_h2, a2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_PRELU_MODEL:\n",
    "    # initialize weights\n",
    "    # input shape is (B, 784)\n",
    "    w_h_prelu = init_weights((784, 625))\n",
    "    a1 = init_weights((batch_size,625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_h2_prelu = init_weights((625, 625))\n",
    "    a2 = init_weights((batch_size,625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_o_prelu = init_weights((625, 10))\n",
    "    # output shape is (B, 10)\n",
    "\n",
    "    optimizer = RMSprop(params=[w_h_prelu, w_h2_prelu, w_o_prelu,a1,a2])\n",
    "\n",
    "    train_loss_prelu = []\n",
    "    test_loss_prelu = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_PRELU_MODEL:\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            # our model requires flattened input\n",
    "            x = x.reshape(batch_size, 784).to(device)\n",
    "            # feed input through model\n",
    "            noise_py_x = prelu_model(x, w_h_prelu, w_h2_prelu, w_o_prelu,a1,a2)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_prelu.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss_prelu[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "                    x = x.reshape(batch_size, 784).to(device)\n",
    "                    noise_py_x = prelu_model(x, w_h_prelu, w_h2_prelu, w_o_prelu,a1,a2)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            test_loss_prelu.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss_prelu[-1]:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_PRELU_MODEL:\n",
    "    plt.plot(np.arange(n_epochs + 1), train_loss_prelu, label=\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 2), test_loss_prelu, label=\"Test\")\n",
    "    plt.title(\"Train and Test Loss over Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET:\n",
    "    hyperparams = {\n",
    "        'length': 3,\n",
    "        'f': [32,64,128],\n",
    "        'pic_in': [1,32,64],\n",
    "        'k_x': [5,5,3],\n",
    "        'k_y': [5,5,3]\n",
    "    }\n",
    "\n",
    "    weight_vectors = [init_weights((hyperparams['f'][i],hyperparams['pic_in'][i],hyperparams['k_x'][i],hyperparams['k_y'][i])) for i in range(3)]\n",
    "\n",
    "    number_of_output_pixel = 128 # chat gpt answer\n",
    "    # hidden layer with 625 neurons\n",
    "    w_h2 = init_weights((number_of_output_pixel, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_o = init_weights((625, 10))\n",
    "    # output shape is (B, 10)\n",
    "\n",
    "    optimizer = RMSprop(params=[w_h2,w_o,*weight_vectors])\n",
    "\n",
    "    train_loss_lenet = []\n",
    "    test_loss_lenet = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leNet(X,weight_vectors,w_h2,w_o,p_drop_input):\n",
    "    '''\n",
    "    x: (batch,1,28,28)\n",
    "    '''\n",
    "    conv1 = rectify(conv2d(X,weight_vectors[0]))\n",
    "    subsampling_layer = max_pool2d(conv1,(2,2))\n",
    "    out_layer = dropout(subsampling_layer,p_drop_input)\n",
    "\n",
    "    conv2 = rectify(conv2d(out_layer,weight_vectors[1]))\n",
    "    subsampling_layer_2 = max_pool2d(conv2,(2,2))\n",
    "    out_layer_2 = dropout(subsampling_layer_2,p_drop_input)\n",
    "\n",
    "    conv3 = rectify(conv2d(out_layer_2,weight_vectors[2]))\n",
    "    subsampling_layer_3 = max_pool2d(conv3,(2,2))\n",
    "    out_layer_3 = dropout(subsampling_layer_3,p_drop_input)\n",
    "\n",
    "    # print(out_layer_3.shape)\n",
    "\n",
    "    flattened_out = out_layer_3.reshape(batch_size,-1)\n",
    "\n",
    "    # print(flattened_out.shape)\n",
    "\n",
    "    h2 = rectify(flattened_out @ w_h2)\n",
    "    pre_softmax = h2 @ w_o\n",
    "\n",
    "    return pre_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lenet(image,filters):\n",
    "    '''\n",
    "    plots:\n",
    "    - original test image\n",
    "    - image after 3 filters from the conv1 applied to it (in parallel)\n",
    "    - filter weights as images\n",
    "    '''\n",
    "    # Perform convolution using the selected filters\n",
    "    conv_results = []\n",
    "    for i in range(3):\n",
    "        conv_result = conv2d(image.unsqueeze(0), filters[i].unsqueeze(0))\n",
    "        conv_results.append(conv_result.cpu().squeeze().detach().numpy())\n",
    "\n",
    "    # Convert the image and filters to numpy arrays for plotting\n",
    "    image_np = image.cpu().squeeze().numpy()\n",
    "    filters_np = filters.detach().cpu().squeeze().numpy()\n",
    "\n",
    "    print(filters_np.shape)\n",
    "\n",
    "    # Plot the results\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "\n",
    "    # Plot the filter weights and convolved images\n",
    "    for i in range(3):\n",
    "        # Plot the original image\n",
    "        axs[i, 0].imshow(image_np, cmap='gray')\n",
    "        axs[i, 0].set_title(\"Original Image\")\n",
    "\n",
    "\n",
    "        # Plot filter weights\n",
    "        axs[i, 1].imshow(filters_np[i], cmap='viridis')\n",
    "        axs[i, 1].set_title(f\"Filter {i + 1} Weights\")\n",
    "\n",
    "        # Plot convolved image\n",
    "        axs[i, 2].imshow(conv_results[i], cmap='viridis')\n",
    "        axs[i, 2].set_title(f\"Convolution with Filter {i + 1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET:\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            # our model requires flattened input\n",
    "            x = x.to(device)\n",
    "            # feed input through model\n",
    "            noise_py_x = leNet(x,weight_vectors,w_h2,w_o,0.5)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_lenet.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss_lenet[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "                    x = x.to(device)\n",
    "                    noise_py_x = leNet(x,weight_vectors,w_h2,w_o,0)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            test_loss_lenet.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss_lenet[-1]:.2e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET:\n",
    "    plt.plot(np.arange(n_epochs + 1), train_loss_lenet, label=\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 2), test_loss_lenet, label=\"Test\")\n",
    "    plt.title(\"Train and Test Loss over Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET:\n",
    "    data_iter = iter(test_dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    image = images[25]\n",
    "    filters = weight_vectors[0][:3]\n",
    "\n",
    "    plot_lenet(image.to(device),filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2: Applying a random linear shift to the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomLinearShift(object):\n",
    "    def __init__(self, shift_range=(-0.5, 0.5)):\n",
    "        self.shift_range = shift_range\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        shift = np.random.uniform(self.shift_range[0], self.shift_range[1])\n",
    "        return tensor + shift\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + f'(shift_range={self.shift_range})'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "transform_extended = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,)),\n",
    "    RandomLinearShift(shift_range=(-0.5,0.5))\n",
    "]) \n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "    ]),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET_SHIFT:\n",
    "    hyperparams = {\n",
    "        'length': 3,\n",
    "        'f': [32,64,128],\n",
    "        'pic_in': [1,32,64],\n",
    "        'k_x': [5,5,3],\n",
    "        'k_y': [5,5,3]\n",
    "    }\n",
    "\n",
    "    weight_vectors = [init_weights((hyperparams['f'][i],hyperparams['pic_in'][i],hyperparams['k_x'][i],hyperparams['k_y'][i])) for i in range(3)]\n",
    "\n",
    "    number_of_output_pixel = 128 # chat gpt answer\n",
    "    # hidden layer with 625 neurons\n",
    "    w_h2 = init_weights((number_of_output_pixel, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_o = init_weights((625, 10))\n",
    "    # output shape is (B, 10)\n",
    "\n",
    "    optimizer = RMSprop(params=[w_h2,w_o,*weight_vectors])\n",
    "\n",
    "    train_loss_lenet_shift = []\n",
    "    test_loss_lenet_shift = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET_SHIFT:\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            # our model requires flattened input\n",
    "            x = x.to(device)\n",
    "            # feed input through model\n",
    "            noise_py_x = leNet(x,weight_vectors,w_h2,w_o,0.5)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_lenet_shift.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss_lenet_shift[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "                    x = x.to(device)\n",
    "                    noise_py_x = leNet(x,weight_vectors,w_h2,w_o,0.0)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y.to(device), reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            test_loss_lenet_shift.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss_lenet_shift[-1]:.2e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET_SHIFT:\n",
    "    plt.plot(np.arange(n_epochs + 1), train_loss_lenet_shift, label=\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 2), test_loss_lenet_shift, label=\"Test\")\n",
    "    plt.title(\"Train and Test Loss over Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_LENET_SHIFT:\n",
    "    data_iter = iter(test_dataloader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    image = images[25]\n",
    "    filters = weight_vectors[0][:3]\n",
    "\n",
    "    plot_lenet(image.to(device),filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if ALL:\n",
    "\n",
    "    data = {\n",
    "        'architecture': ['base', 'dropout', 'prelu', 'lenet', 'lenet + shift'],\n",
    "        'test_loss': [test_loss[-1],test_loss_drop[-1],test_loss_prelu[-1],test_loss_lenet[-1],test_loss_lenet_shift[-1]],\n",
    "        'train_loss': [train_loss[-1],train_loss_drop[-1],train_loss_prelu[-1],train_loss_lenet[-1],train_loss_lenet_shift[-1]]\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    print(\"Number of epochs: {}\".format(n_epochs))\n",
    "    print(\"Learning rate: {}\".format(1e-3))\n",
    "    print(\"Batch size {}\".format(batch_size))\n",
    "    print(\"Dropout rate (dropout model + lenet + lenet with shift) {}\".format(0.5))\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that regularization techniques (dropout, random shift) help with overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
